{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f49076c0-50c7-4f43-96c7-2e381abc50d2",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "## XGBoost, LightGBMとは？\n",
    "\n",
    "gradient boosting をC++で実装したものがXGBoost であり、それをベースにしてより軽量な実装を行ったものが LightGBMである。どちらも実装面での話であるため、以下で議論する Gradient Boosting Tree (GBT) を理解すればok。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050847f4-8800-48ae-930f-4c268d01b60e",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "\n",
    "- https://arxiv.org/pdf/1603.02754.pdf\n",
    "- https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/\n",
    "- https://xgboost.readthedocs.io/en/latest/tutorials/model.html\n",
    "- https://towardsdatascience.com/xgboost-mathematics-explained-58262530904a\n",
    "\n",
    "## XGBoostとは？\n",
    "\n",
    "スケーラビリティの高い boosting 用の学習アルゴリズムである。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc034ba8-ebae-4b1e-83a8-b548770ee4d6",
   "metadata": {},
   "source": [
    "## Boosting について\n",
    "\n",
    "アンサンブル学習の一つで、一つのデータから一つのモデルを一回きり作成するのではなく、weak leaner (wark classifier, 弱学習器）を何度も作成し、それらの多数決で最終的な予測を行うものである。boosted tree はいわゆる直列で tree を作成するモデルであり（あるイテレーションで作成した tree が次のイテレーションの tree の判断に影響を与える）、bagging とは異なる概念を持つ。歴史的には以下のようになっている。\n",
    "\n",
    "- Random Forest (Breiman, 1997)\n",
    "- Gradient Tree Boosting (Friedman, 1999)\n",
    "- Gradient Tree Boosting with Regulatization --> XGBoost\n",
    "\n",
    "tree 系は精度がよく、使いやすく、解釈しやすいというメリットがある一方で、すぐに過学習してしまうし、学習スピードに難があった。\n",
    "\n",
    "### AdaBoost\n",
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97eb343-e51c-486c-b780-dac2bc691b09",
   "metadata": {},
   "source": [
    "# Decision Tree Ensamble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904898fa-9d60-4102-a972-5ed67a3492df",
   "metadata": {},
   "source": [
    "サンプル数$n$、特徴量個数 $m$ のデータが与えられたとする。\n",
    "\n",
    "$$\n",
    "\\mathcal{D}=\\{(\\boldsymbol{x}_i, y_i)\\}~~,\\text{where}~|\\mathcal{D}|=n,~\\boldsymbol{x}_i\\in R^m,~y_i\\in R\n",
    "$$\n",
    "\n",
    "ここで、$K$ 本のツリーを作成して、それらを用いて予測値を算出する場合を考える（ex. 特徴量全てを使うのではなく、$K$通りの組み合わせでツリーを作るとか）。tree ensamble とは各ツリーの結果の線形結合で一つの予測値を算出することを指す。\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = \\sum_{k=1}^K f_k(x_i),~~f_k \\in \\mathcal{F}\n",
    "$$\n",
    "\n",
    "この場合の目的関数は\n",
    "\n",
    "$$\n",
    "\\text{obj} = \\sum_{i=1}^n l(y_i,\\hat{y}_i) + \\sum_{k=1}^K \\Omega(f_k)\n",
    "$$\n",
    "\n",
    "第一項は損失関数であり、第二項は正則化項でモデルが複雑にならないように調整する役割を果たす。ツリー系は前述の通り over fitting しやすいので正則化項が必要だが、もちろんそこにはトレードオフが存在している。\n",
    "\n",
    "- 損失関数を最適化することは、予測性能の高いモデルを組むことに相当する\n",
    "- 正則化項を最適化することは、シンプルなモデルを組むことに相当する。\n",
    "    - シンプル = 予測性能が悪い、ではなく、予測が安定し（例えば outlier に反応しない）信頼のおけるモデルであるという意味を含む。\n",
    "    \n",
    "    \n",
    "ここでモデルを表現するための量を定義する。\n",
    "\n",
    "$$\n",
    "f_t(x) = w_{q(x)},~~w \\in R^T,~q : R^d \\to \\{1,2,...,T\\}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7539e575-0d36-4e07-948f-bb65aa0aead2",
   "metadata": {},
   "source": [
    "# Tree Boosting\n",
    "\n",
    "目的関数は以下のように再定義する：\n",
    "\n",
    "$$\n",
    "\\text{obj} = \\sum_{i=1}^n l(y_i, \\hat{y}_i^{(t)}) + \\sum_{i=1}^t \\Omega(f_i)\n",
    "$$\n",
    "\n",
    "ここで添え字 $t$ はステップ(or イテレーション回数）を表し、$t$ 回目にツリーを作成するときの目的関数である。\n",
    "\n",
    "## Additive Training\n",
    "\n",
    "ツリーを学習するときに、そのツリーの構造もパラメータで調整できるが、これは目的関数から最小化するのは困難である。ツリーの構造は即ちモデルの構造であり（DNNであればレイヤー数やレイヤーの種類）、それをイテレーション毎に過去に遡って最適化することは難しい。そこで、additive training （加法性学習）という形で、$t-1$ までで組んだツリーに新たに一本ツリーを追加して、それを持って予測値と目的関数を計算するというものである。つまり、過去までの学習過程を踏まえて現時点でどんなツリーが好ましいかを学習するのである（=直列性）。またこれは、通常の損失関数の最小化ともリンクしており、通常も多変数関数である損失関数を一気に最小化することは困難である。そこで損失関数の勾配を使って（勾配降下法）、最適化を行っているが、それと類似している。\n",
    "\n",
    "$$\n",
    "\\hat{y}_i^{(t)} = \\hat{y}_i^{(t-1)} + f_t(x_i) = \\sum_{k=1}^{t-1} f_k(x_i) + f_t(x_i)\n",
    "$$\n",
    "\n",
    "ここで、$t$ ステップのときの目的関数は以下のように書ける：\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\text{obj}^{(t)} &=&  \\sum_{i=1}^n l(y_i, \\hat{y}_i^{(t)}) + \\sum_{i=1}^t \\Omega(f_i) \\\\\n",
    "&=& \\sum_{i=1}^n l \\left(y_i, \\hat{y}_i^{(t-1)} + f_t(x_i) \\right) + \\sum_{i=1}^t \\Omega(f_i) \n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "ここで、$y_i$ は訓練データセットからすでに知っている目標変数であり、$\\hat{y}_i^{(t-1)}$ は $t-1$ ステップ目までのツリーの線形結合による予測値、$f_t(x_i)$ はこのステップで追加したツリーである。上手いツリーを追加することができれば、目標変数と予測値との誤差 $l$ が小さくなり目的関数の最適化が実現に近づく。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a299edc-dfbc-4846-9586-1eab23c973b5",
   "metadata": {},
   "source": [
    "## Taylor expansion\n",
    "\n",
    "ここで目的関数を、数学的トリックを使ってより解釈しやすいようにする。より正確には、従来使用されている最適化手法を用いるために、目的関数をユークリッド領域の関数に変換する必要があるからである。$f(x)$ の $x_0$ 周りのテイラー展開は：\n",
    "\n",
    "$$\n",
    "f(x) = f(x_0) + f^\\prime(x_0)(x-x_0)+ \\frac{1}{2} f^{\\prime\\prime}(x_0)(x-x_0)^2 + ...\n",
    "$$\n",
    "\n",
    "ここで $x-x_0=\\Delta x$ とすれば\n",
    "\n",
    "$$\n",
    "f(x_0 + \\Delta x) = f(x_0) + f^\\prime(x_0)\\Delta x + \\frac{1}{2} f^{\\prime\\prime}(x_0) \\Delta x^2 + ...\n",
    "$$\n",
    "\n",
    "上記の目的関数の第一項をテイラー展開すると（$x_0=\\hat{y}_i^{(t-1)}$、$\\Delta x = f_t(x_i)$）：\n",
    "\n",
    "$$\n",
    "l \\left( y_i, \\hat{y}_i^{(t-1)} + f_t(x_i) \\right) \\simeq l(y_i, \\hat{y}_i^{(t-1)}) ~+~ \\frac{\\partial}{\\partial \\hat{y}_i^{(t-1)}} l(y_i, \\hat{y}_i^{(t-1)}) f_t(x_i) ~+~ \\frac{1}{2}\\frac{\\partial^2}{\\partial^2 \\hat{y}_i^{(t-1)}} l(y_i, \\hat{y}_i^{(t-1)}) f_t(x_i)^2 + ...\n",
    "$$\n",
    "\n",
    "ゆえに目的関数は、\n",
    "\n",
    "$$\n",
    "\\text{obj}^{(t)} = \\sum_{i=1}^n \\left[ l(y_i, \\hat{y}_i^{(t-1)}) + g_if_t(x_i) + \\frac{1}{2} h_if_t^2(x_i) \\right] + \\Omega(f_t) + \\text{Const.}\n",
    "$$\n",
    "\n",
    "ここで、$g_i$ と $h_i$ はそれぞれ、損失関数の１階・２階微分（勾配）である。\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "g_i &=& \\partial_{\\hat{y}_i^{(t-1)}}~l \\left(y_i, \\hat{y}_i^{(t-1)} \\right) \\\\\n",
    "h_i &=& \\partial^2_{\\hat{y}_i^{(t-1)}}~l \\left(y_i, \\hat{y}_i^{(t-1)} \\right) \\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "ステップ $t$ において、$t-1$ までのステップで計算されているものは定数として扱えるので、定数項を除外すると以下のようになる（正則化項も $t$ だけを取り出していることに留意）：\n",
    "\n",
    "$$\n",
    "\\tilde{\\mathcal{L}}^{(t)} = \\sum_{i=1}^n \\left[ g_if_t(x_i) + \\frac{1}{2} h_if_t^2(x_i) \\right] + \\Omega(f_t)\n",
    "$$\n",
    "\n",
    "勾配 $g_i,h_i$ の値自体はステップ $t-1$ までで計算されている係数として考えることができるので、この目的関数の値を左右する量は $f_t(x_i)$ = ツリーをどう設計するか、だけである。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7847adc4-3585-4ee6-82ad-d5bcadf5dc85",
   "metadata": {},
   "source": [
    "## Split finding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0bec40-b1e3-477f-a43c-22122ee8b77d",
   "metadata": {},
   "source": [
    "ここで、$j$ 番目のリーフに分類されたデータのインデックス集合を $I_j = \\{ i|q(x_i)=j \\}$ で表す。$q(x)$ はデータ $x$ がどのリーフに分類されたかを返す関数である。これによって、目的関数は、リーフに関するインデックスの和として次のようにまとめることができる：\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\mathcal{L}^{(t)} &=& \\sum_{i=1}^n \\left[ q_if_t(x_i) + \\frac{1}{2}h_if_t^2 \\right] + \\gamma T + \\frac{1}{2} \\sum_{j=1}^T w_j^2 \\\\\n",
    "&=& \\sum_{j=1}^T \\left[ \\left(\\sum_{i\\in I_j}q_i\\right)w_j + \\frac{1}{2} \\left(\\sum_{i\\in I_j} h_i + \\lambda\\right)w_j^2\\right] + \\gamma T\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "固定された構造 $q(x)$ に関して、$j$ 番目のリーフの重みの最適解は\n",
    "\n",
    "$$\n",
    "w_j^* = \\frac{\\sum_{i\\in I_j}q_i}{\\sum_{i\\in I_j} h_i + \\lambda}\n",
    "$$\n",
    "\n",
    "また最適な目的関数は、\n",
    "\n",
    "$$\n",
    "\\mathcal{L}^{(t)} (q) = -\\frac{1}{2} \\sum_{j=1}^T \\frac{(\\sum_{i\\in I_j}q_i)^2}{\\sum_{i\\in I_j} h_i + \\lambda} + \\gamma T\n",
    "$$\n",
    "\n",
    "と表される。整理すると\n",
    "\n",
    "- $g_i, h_i$ は定数係数であり、$t-1$ ステップまでの試行で決定されているものである\n",
    "- $\\lambda$ も定数係数であり、一種のhyperparameter である\n",
    "- そのため、最適解 $w_j^*$ は構造 $q$ の関数である\n",
    "\n",
    "\n",
    "しかし、ここで問題なのが、全ての木構造 $q$ に関して走査することが通常は不可能であるということである。その代わり、「greedy algorithm（貪欲法）」を使って、single leaf （これはroot nodeとして解釈できる）から初めて枝を追加していく方法が取られる。一度スプリットすると、$I_L$ と $I_R$（全てのサンプルが左か右のノードに分かれる）という状態ができ、$I = I_L \\cup I_R$ であるとする。これを用いることで、もともとの損失関数の値からどう変化した（loss reduction） が計算することができる：\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{split}} = \\frac{1}{2} \\left[ \n",
    "\\frac{(\\sum_{i\\in I_L}q_i)^2}{\\sum_{i\\in I_L} h_i + \\lambda}\n",
    "+\\frac{(\\sum_{i\\in I_R}q_i)^2}{\\sum_{i\\in I_LR} h_i + \\lambda}\n",
    "- \\frac{(\\sum_{i\\in I}q_i)^2}{\\sum_{i\\in I} h_i + \\lambda}\n",
    "\\right]\n",
    "- \\gamma\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
